 ------
Swagr
 ------

Swagr

 Swagr (<<S>>ystem <<W>>ide <<A>>ctivity <<Gr>>aphs) is a tool for analyzing service utilization.

[../../images/swagr/swagr.png]

 <<Basic Swagr Client Output>>

* Using Swagr

 The Swagr UI presents a series of options for configuring your request, depending on how you want to inspect the system.

 Each of the combo boxes has a tooltip if you hover over it.

 The toolbar across the top of the page presents the following filters/options, from left to right:

     * <<System Category>> Only Louie Requests is still supported. Changing the System Category changes the rest of the combo boxes so this document focuses on Louie Requests.

     * <<Chart Mode>>  Top X is the default, which displays the top X requests over a range of dates. Service Totals is per-service totals. System totals is the entire system. Deprecated Service Calls is dependent on annotated methods in the java code.

     * <<Top X>> The numeric component for the Top X Chart Mode. ie; '10' would indicate Top 10 Requests

     * <<Services>> The Louie-based service to filter on.

     * <<Location>> Really, the specific server you want to see. It used to be that we only supported one Louie instance per location, hence the division by location.

     * <<Data Type>> The requesting language

     * <<Timeshift>> A deprecated mode (Daylight Matched should always be used)

     * <<Start Date>> Beginning of request range

     * <<End Date>> End of request range

     * <<Calculation Type>> The X-Axis definition. ''Count Volume'' is the number of requests. ''Duration'' is count volume * average duration in ms. ''Load'' is count volume * average bytes. 

     * <<Remove Weekends>> An option to prevent weekend dates from being included in results.

     * <<DRAW>> Draw it! 


 In Swagr, a unique request is defined by the System:Service:Method(Params), ie a specific method signature like louie:queue:getHostUsage(HostUsageFilterBPB filter). (Note that "System" is likely always Louie)

 Once the chart has been drawn, the rendered image is also interactive. 

 Swagr displays data points which intersect with the grid. So, if you hover your mouse over one of the colored lines where it intersects with a vertical grid line, you will see a tooltip displaying relevant information. If you right click on the same point, it will display an option to view more data, or remove the target request. Removing will cause the chart to be redrawn without that request. View more data opens a new window inside the application which shows composition information as well as moving averages for the target request.

[../../images/swagr/swagr_hover.png]

 <<An example tooltip>>

[../../images/swagr/swagr_rightclick.png]

 <<Right click menu>>

[../../images/swagr/swagr_mini_view.png]

 <<Swagr composition graph>>


 * Understanding the chart

 Typically, the most useful feature of a graphical representation of the data is seeing sharp upticks, or large disparities between services or methods. Swagr is meant to show you how your services are being used, and how each service and method stacks up against each other. Because the Y Axis is time, it's also possible to chart activity over a very long period of time (Beware though, that the browser can only render so many DOM elements before it will just crash). If you wish to look at long periods of time, I suggest first narrowing everything you can by adjusting your parameters.

 The inner-charts visible by selecting "View more data..." from the right click menu are meant to show compositional data as stacked charts. For instance, the location button will display the utilization across all known locations, as a stacked chart. Also visible in this window is an option to view a moving average. This will display a line representing the 30 day average drawn across the normal request, useful for inspecting growth or decline over time. For a given day point, the value is computed as the average of the previous thirty days. 

* Administrating Swagr

 Swagr exists as a set of components. At the bottom is a cron executing syslogscan.py which is the log crawler. The crawler coordinates with and stores information in a set of MySQL tables. 

 It tracks which hosts and which log files it needs to scan, and will perform a retry if a log isn't scanned, for up to three days after the initial attempt. syslogscan.py downloads or rsync's the target files onto the local machine, and parses the complete text. The information is then compressed into hourly data points which are stored in the stats_hourly table. stats_daily, top_stats_data, top_stats_service_data, top_stats_system, top_stats_type, and moving_avgs are all derived from that initial set. The Swagr louie-based service is driven by these tables and provides a (somewhat confusing) interface into the data. The service is specifically designed to drive the charts so the organization of the returned data and the request format can be somewhat hard to understand. If you intend to use the Swagr service data to drive something else, the SwagrOptionsPB has notes for each item in the swagr.proto file.

 * <<Setting things up>>
 
 1. A MySQL database should be set up and tables created using the swagr.mysql file 
 
 2. Your request log should be configured as per the archetype's initial log4j2.xml so that the formatting is consistent.

 3. syslogscan.py -s should be run daily, ideally with a cron, to pull the request logs from all hosts currently running louie-based services. The cron can be executed multiple times a day if you don't know how much activity your system will experience (and therefore when the logs will be rolled), since the crawler will only ever scan the same file once.

 4. Your container (we use glassfish in this example) must be configured to supply a connection pool to your retrospect database under the JNDI name "jdbc/SYSstatistics"

 5. ~/swagr > mvn clean install 

 6. Deploy the Server as well as the Client to your container

 7. If you added a host and your cron has already run, you should be able to view some data.

 * <<Adding a new host>>

 Since the log crawler is driven by the db tables, it's possible to add new hosts (identified as locations) at any time as your requirements change. *Please note that the crawler can rsync a file directly if it has access permissions, otherwise it can download the file through glassfish. If you wish for the crawler to download through glassfish you must perform a couple additional steps listed below.

 Here is an example of how I added a host:

+-----+
INSERT INTO host (systemid,name,logdir,filepattern,location,web) VALUES (1,'10.100.16.29','http://10.100.16.29:8080/logs','request.log.%Y-%m-%d',1);
+-----+

 the logdir can represent either the url it downloads from, or the path on disk if it's meant to rsync the file. This field must also coordinate with the 'web' column, which indicates whether the crawler should try to download or rsync the file.
 

 If the log file should be downloaded through glassfish, you'll have to create a symlink in the glassfish "docroot" directory pointing to your log directory, and then change an admin feature.

+-----+
/usr/randh/glassfish4/glassfish/domains/domain1/docroot > ln -s ../logs/loup logs
+-----+

 You must then configure glassfish to allow symlinks. From the admin page (port 4848 of the relevant glassfish instance), navigate to Configurations->Virtual Servers->server and add an additional property at the bottom called "allowLinking" and set it to "true" then restart that glassfish instance.

 * <<Disabling an existing host>>

 If you plan to retire a certain host (location), you do not have to wipe the existing data for it. You can simply update the "host" table in the "retrospect" database to set active = 0.

+-----+
UPDATE host SET active = 0 WHERE id = <host id>;
+-----+

* Additional things

 Though Swagr runs consistently, it's pretty messy behind the scenes. Something to be aware of is that the crawler does not currently purge anything, ever.


* Good luck and happy charting!

