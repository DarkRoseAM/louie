 ------
Swagr
 ------

Administrating Swagr

 Swagr exists as a set of components. At the bottom is a cron executing swagrscan.py which is the log crawler. The crawler coordinates with and stores information in a set of MySQL tables. 

 It tracks which hosts and which log files it needs to scan, and will perform a retry if a log isn't scanned, for up to three days after the initial attempt. swagrscan.py downloads or rsync's the target files onto the local machine, and parses the complete text. The information is then compressed into hourly data points which are stored in the stats_hourly table. stats_daily, top_stats_data, top_stats_service_data, top_stats_system, top_stats_type, and moving_avgs are all derived from that initial set. The Swagr louie-based service is driven by these tables and provides a (somewhat confusing) interface into the data. The service is specifically designed to drive the charts so the organization of the returned data and the request format can be somewhat hard to understand. If you intend to use the Swagr service data to drive something else, the SwagrOptionsPB has notes for each item in the swagr.proto file.

 * <<Setting things up>>
 
 1. A MySQL database should be set up and tables created using the swagr.mysql file 
 
 2. Your request log should be configured as per the archetype's initial log4j2.xml so that the formatting is consistent.

 3. swagrscan.py -s should be run daily, ideally with a cron, to pull the request logs from all hosts currently running louie-based services. The cron can be executed multiple times a day if you don't know how much activity your system will experience (and therefore when the logs will be rolled), since the crawler will only ever scan the same file once.

+-----+
# swagr daily log scanner
00 1 * * * /usr/local/crons/swagr/swagrscan.py -s >> /usr/local/crons/swagr/swagrscan.log 2>&1
+-----+

 4. Your container (we use glassfish in this example) must be configured to supply a connection pool to your swagr database under the JNDI name "jdbc/swagrDS"

 5. ~/swagr > mvn clean install 

 6. Deploy the Server as well as the Client to your container

 7. If you added a host and your cron has already run, you should be able to view some data.

 * <<Adding a new host>>

 Since the log crawler is driven by the db tables, it's possible to add new hosts (identified as locations) at any time as your requirements change. *Please note that the crawler can rsync a file directly if it has access permissions, otherwise it can download the file through glassfish. If you wish for the crawler to download through glassfish you must perform a couple additional steps listed below.

 Here is an example of how I added a host:

+-----+
INSERT INTO host (systemid,name,logdir,filepattern,location,web) VALUES (1,'10.100.16.29','http://10.100.16.29:8080/logs','request.log.%Y-%m-%d',1);
+-----+

 the logdir can represent either the url it downloads from, or the path on disk if it's meant to rsync the file. This field must also coordinate with the 'web' column, which indicates whether the crawler should try to download or rsync the file.
 

 If the log file should be downloaded through glassfish, you'll have to create a symlink in the glassfish "docroot" directory pointing to your log directory, and then change an admin feature.

+-----+
/usr/randh/glassfish4/glassfish/domains/domain1/docroot > ln -s ../logs/<project name> logs
+-----+

 You must then configure glassfish to allow symlinks. From the admin page (port 4848 of the relevant glassfish instance), navigate to Configurations->Virtual Servers->server and add an additional property at the bottom called "allowLinking" and set it to "true" then restart that glassfish instance.

 * <<Disabling an existing host>>

 If you plan to retire a certain host (location), you do not have to wipe the existing data for it. You can simply update the "host" table in the "swagr" database to set active = 0.

+-----+
UPDATE host SET active = 0 WHERE id = <host id>;
+-----+

* Additional things

 Though Swagr runs consistently, it's pretty messy behind the scenes. Something to be aware of is that the crawler does not currently purge anything from that database, ever.


* Good luck and happy charting!
